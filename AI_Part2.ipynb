{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt6Lns2bL1+oSZWi+I6v7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marinarhianna/python-tutorials/blob/main/AI_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teaching our AI how to think üëæ\n",
        "\n",
        "\n",
        "---\n",
        "In this notebook, we will see how to train and test our network.\n",
        "\n",
        "\n",
        "\n",
        "*   Read through the descriptions carefully!\n",
        "*   This is advanced stuff, so ask Marina if anything is confusing üò∏\n",
        "*   Run each cell, and at the end there is a challenge for you to try some things out to improve our AI's performance.\n",
        "*   Happy coding!\n"
      ],
      "metadata": {
        "id": "SrUlZ2sy_ysy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code from previous notebook\n",
        "This cell contains all the code from the previous notebook in one cell. We need to run this before doing anything else!\n",
        "\n",
        "I have made the training and testing sets a bit larger here, just so the network has enough data to learn from, and also implemented some normalization to the datasets, to help the network learn patterns. In machine learning, it is very common to normalize data."
      ],
      "metadata": {
        "id": "KFCFjcgaBh9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W_KCdAWu_lNT"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create training and testing sets\n",
        "X_train = [\n",
        "    [1, 2, 3, 4], [4, 3, 2, 1], [2, 2, 3, 3], [1, 1, 2, 2],  # small numbers\n",
        "    [100, 200, 300, 400], [400, 300, 200, 100], [150, 150, 200, 200], [120, 130, 140, 150]  # large numbers\n",
        "]\n",
        "y_train = [0, 0, 0, 0, 1, 1, 1, 1]   # labels\n",
        "\n",
        "X_test = [\n",
        "    [2, 3, 4, 5], [3, 2, 1, 0], [180, 190, 200, 210], [220, 230, 240, 250]\n",
        "]\n",
        "y_test = [0, 0, 1, 1]\n",
        "\n",
        "# normalize all values to be between 0 and 1\n",
        "X_train = np.array(X_train) / 400.0\n",
        "X_test = np.array(X_test) / 400.0\n",
        "\n",
        "# convert to tensor form\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# define function\n",
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 4, 2)\n",
        "        self.fc1 = nn.Linear(4 * 3, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 - Create a model using our blueprintüó∫\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zfiOGcw6KkdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model by calling our class\n",
        "model = MyNetwork()\n",
        "\n",
        "# check out the architecture of our network\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ZIBP-7XPKh5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above print statement allows us to quickly see the structure of our neural network."
      ],
      "metadata": {
        "id": "2XtnsvZkLTw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Define network parameters üñä\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are a few important **parameters** of our network that we need to set before we can train it.\n",
        "\n",
        "Some important things to remember:\n",
        "\n",
        "\n",
        "*   A neural network is trained by assigning **weights** to pieces of information, to tell the network which **features** are important.\n",
        "*   These weights start off **random**.\n",
        "*   As information passes **forward** through the network, it makes a random **guess**, and then checks the **real** label.\n",
        "*   Depending on how close the guess is to the **truth**, it feeds the information **backwards** and **updates** the weights accordingly.\n",
        "*   This process is repeated until the **predictions match the truth**.\n",
        "\n",
        "So, let's take a look at the network parameters that help us do this:\n",
        "\n",
        "\n",
        "*   **Loss function**: this is how we measure the difference between the **predictions** our network makes and the **real** labels. It tells us **how far away we are from the truth**. A perfect prediction would have a loss of zero, and a bad prediction would be much higher loss.\n",
        "*   **Optimizer**: this is how we **update the weights** to improve our network's performance. The optimizer calculates the **gradient** of the loss function to find out what direction we need to go down to make the loss smaller.\n",
        "* **Learning rate**: this sets how much we are allowed to change the weights by. We have to tell the optimizer how many steps in either direction it is allowed to take once it has calculated the gradient of the loss.\n",
        "\n",
        "Now, how do we actually set these parameters in code form?\n",
        "\n",
        "\n",
        "*   We name the loss function `criterion`, and call a function from our `nn` package called `CrossEntropyLoss`.\n",
        "    * Cross-entropy is just an equation where if the prediction is correct, it outputs a zero. If you're curious to see what this equation actually looks like, ask Marina!  \n",
        "*   Recall that earlier, we imported `torch.optim` as `optim` (the same way that we imported `torch.nn` as `nn`).\n",
        "    * To set up our `optimizer` parameter, we call a function named `Adam` from our `optim` package.\n",
        "    * `Adam` is short for \"Adaptive Moment Estimation\" and it works by cleverly tracking the magnitude of the current gradient, as well as previous gradients. The maths behind `Adam` is quite nasty, but luckily Python has it all built in already so we can just call it as a function.\n",
        "\n",
        "Within the `optim.Adam()` function, we need to tell it:\n",
        "\n",
        "\n",
        "1.   **Where to find our neural network:** earlier, we created a shortcut to our network by naming it `model = MyNetwork()`. Remember that `MyNetwork()` is what we named our `class` at the very beginning. So, we can say `model.parameters()` which tells Python that `model` is the network that we are defining the parameters for.\n",
        "2.   **Learning rate**: learning rate is abbreviated to `lr=`, where common numbers to choose are 0.1, 0.01, 0.0001, etc. It is often a case of trial and error to see which learning rate produces the best performing network.\n"
      ],
      "metadata": {
        "id": "zoyAp3N-L092"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set our loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# set our optimizer and learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "xqbDL_BFKjQJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 - Train our AI üèã\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Before we start training, let's go over some terminology.\n",
        "\n",
        "Remember that information gets passed **forwards** and **backwards** through the network.\n",
        "* In the **forward** pass, the network makes a **prediction** and calculates the **loss** function.\n",
        "* In the **backwards** pass, the optimizer calculates the **gradient** of the loss and takes a step in a particular direction to make the loss smaller.\n",
        "\n",
        "To train a neural network, we pass the training dataset through the network multiple times.\n",
        "An **epoch** is how many times we pass our training dataset through the network.\n",
        "\n",
        "In the backwards pass, we need to make sure that the optimizer calculates the gradient of the loss **from scratch** each time it loops through the training set. Otherwise, the gradients layer on top of each other and the network gets muddled about how wrong the predictions are. To do this, we tell Python to `.zero_grad()` on the `optimizer`, to make sure we are starting from zero at each loop."
      ],
      "metadata": {
        "id": "ttvjE3af49zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a chance to introduce you to a couple of ways to make a print statement more helpful and neat.\n",
        "\n",
        "In our training loop, we want to print out at each epoch what the loss is, so we can see whether the loss decreases as the network runs through the training dataset. Remember, a loss of zero is a perfect guess, so we want the loss to decrease as much as possible.\n",
        "\n",
        "To retrieve the loss from the code, we need to say `loss.item()` to find the actual number.\n",
        "\n",
        "From the previous notebooks, one way to print this information would be:\n",
        "\n",
        "`print(\"Epoch:\", epoch, \"Loss:\", loss.item())`\n",
        "\n",
        "However, the loss might be a very long number. It is helpful to tell Python to round this to a certain number of decimal places.\n",
        "\n",
        "Say we wanted to round a variable to 3 decimal places. We would add `:3f` to the end of it.\n",
        "\n",
        "Also, in the print statement above, there are a lot of quotation marks and commas. This can be tedious to type out.\n",
        "\n",
        "There is a handy shortcut for this called an f-string. By putting an `f` before the quotation marks, it means that we only need one set of quotation marks, and no commas at all. To let Python know when we want to insert a variable, instead of separating them with commas, we can just put the variable inside curly brackets.\n",
        "\n",
        "So, the above print statement can be written as:\n",
        "\n",
        "`print(f\"Epoch: {epoch} Loss: {loss.item():3f}\")`\n"
      ],
      "metadata": {
        "id": "XC5ol2ty_M8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting all of this together, below is an example of a training loop for our neural network."
      ],
      "metadata": {
        "id": "jmP7mCl4BSq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set a number of epochs\n",
        "epochs = 5\n",
        "\n",
        "# create a variable to store losses\n",
        "losses = []\n",
        "\n",
        "# create a loop for training\n",
        "for epoch in range(epochs):\n",
        "    # call torch's training function on our model\n",
        "    model.train()\n",
        "\n",
        "    ### FORWARD PASS:\n",
        "    # make a prediction\n",
        "    y_pred = model(X_train_tensor)\n",
        "    # calculate the loss function\n",
        "    loss = criterion(y_pred, y_train_tensor)\n",
        "\n",
        "    ### BACKWARD PASS:\n",
        "    # make sure the gradients are calculated from scratch\n",
        "    optimizer.zero_grad()\n",
        "    # find out how to improve the loss\n",
        "    loss.backward()\n",
        "    # update the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # add the losses to the variable\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # print progress\n",
        "    print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "o8aziDMeT1hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 - Testing our AI üß™\n",
        "\n",
        "---\n",
        "\n",
        "Now, let's see how this initial model performs on data it has never seen before.\n",
        "\n",
        "Some things to note before we test our AI:\n",
        "\n",
        "* We need to tell our model not to calculate any gradients this time, because we are no longer updating the weights. To do this, we can specify\n",
        "`with torch.no_grad():` before we do our testing.\n",
        "* The predictions that our network makes are stored in a variable `y_pred_test`. These predictions are structured as a score for each label, with the higher score being the more confident prediction.\n",
        "    * For example, we have labels *small* and *big*, stored as `[0, 1]` where 0 is small and 1 is big. So, if the model was predicting that this item was labelled as big (or 1), an example of the prediction would be `[0.4, 3.9]`, where the 0 is scored at 0.4 and the 1 is scored at 3.9. Since 3.9 is higher than 0.4, this would mean the model is more confident that the item is labelled as 1, meaning it is big.\n",
        "    * To retrieve the prediction from this list of scores, we need to tell Python to pick the highest number out of the list. To do this, we call a function `torch.argmax()`, which picks out the maximum *argument* (aka item or number) within a list."
      ],
      "metadata": {
        "id": "HhKCekgRFQhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start the evaluation process\n",
        "model.eval()\n",
        "\n",
        "# do not calculate any gradients\n",
        "with torch.no_grad():\n",
        "    # make a prediction\n",
        "    y_pred_test = model(X_test_tensor)\n",
        "    # get the highest scoring prediction\n",
        "    predicted_labels = torch.argmax(y_pred_test, dim=1)"
      ],
      "metadata": {
        "id": "b-ZxkeOrDkW7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 - Assess our AI's performance üíÉ\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dbDoyQNcVcMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's break down the network's predictions. We can use `.tolist()` to convert the prediction tensor into a list form. Remember, there were 4 groups in our test set, so the below code shows us what the predictions VS real labels were for each of the 4 groups."
      ],
      "metadata": {
        "id": "vIgI6K0fOHb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show predictions\n",
        "print(\"Predicted:\", predicted_labels.tolist())\n",
        "\n",
        "# show actual labels\n",
        "print(\"Actual:\", y_test)"
      ],
      "metadata": {
        "id": "nwb32m-4NT6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use a special function called `softmax` which converts the confidence scores from earlier into actual probabilities, for us to see how confident the predictions were.\n",
        "\n",
        "`softmax` presents this information to us in tensor form, meaning it will show us a 2x4 **matrix**. A matrix is just a vector with more dimensions!\n",
        "\n",
        "In other words, it will show us each of our 4 test groups, with 2 probabilities in each group. These 2 probabilities are for each label. For example, if it shows us:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "tensor([[0.5, 0.5],\n",
        "        [1, 0],\n",
        "        [0.3, 0.7],\n",
        "        [0, 1]])\n",
        "```\n",
        "\n",
        "This means for the first group, it wasn't more confident in either label. In other words, it took a 50/50 guess.\n",
        "\n",
        "For the second group, it was 100% confident that it was the first label.\n",
        "\n",
        "For the third group, it was more confident that it was the second label.\n",
        "\n",
        "For the last group, it was 100% confident that it was the second label.\n",
        "\n",
        "This is just an example!!!\n",
        "\n",
        "Run the cell below to see what the actual prediction probabilities were.\n"
      ],
      "metadata": {
        "id": "-tzntfe7QO3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print prediction probabilities\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "probabilities = softmax(y_pred_test)\n",
        "print(\"Prediction Probabilities:\")\n",
        "print(probabilities)"
      ],
      "metadata": {
        "id": "3yYPK2LFPz2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a chat about what these probabilities mean for the performance of the network so far. How well is it really learning?"
      ],
      "metadata": {
        "id": "z5EvlXUHSwrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's calculate and print the overall accuracy of our network.\n",
        "\n",
        "Accuracy (%) = (Correct / Total) $\\times$ 100\n",
        "\n",
        "To find the **Correct** number of predictions:\n",
        "\n",
        "* Check which predictions are correct by comparing the predicted labels to the true labels. In Python, if we say `x == y`, it will output a `True` or `False` depending on whether the statement holds. So, if we compare `predicted_labels == y_test_tensor `, it will output something like `[True, True, False, True]`.\n",
        "* We then add these up to get how many were `True` by implementing a function `.sum()`.\n",
        "* We then convert this collection of `True`s into a number by implementing a function `.item() `. Now we have a number for the correct predictions.\n",
        "\n",
        "To find the **Total**:\n",
        "* We just need the length of the group containing the labels: `len(y_test_tensor)`.\n",
        "\n",
        "Then, we can multiply this by 100 to get the percentage.\n"
      ],
      "metadata": {
        "id": "Qcvdo_aFS5me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate correct predictions\n",
        "correct = (predicted_labels == y_test_tensor).sum().item()\n",
        "\n",
        "# get total\n",
        "total = len(y_test_tensor)\n",
        "\n",
        "# calculate accuracy\n",
        "accuracy = correct / total\n",
        "\n",
        "# print accuracy\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "KVWgfd88P1uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a deeper look at how our AI is really learning.\n",
        "\n",
        "It is useful for us to visualise how the predictions are improving with each pass over the training dataset."
      ],
      "metadata": {
        "id": "D32UK6HJVSeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot Loss against Epoch\n",
        "plt.plot(losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4M8HN2jP3CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fun Plot: Confusion Matrix üßê\n",
        "\n",
        "A confusion matrix is another way to visualise the network's performance.\n",
        "\n",
        "*   It compares the prediction (x-axis) with the real label (y-axis).\n",
        "*   Run the cells and have a look at the plot.\n",
        "*   Ask Marina if you feel confused!! (no pun intended...)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9q7Q0COW8Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import some extras\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "9fqxa9pqXK00"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create confusion matrix -- don't worry too much about the details!\n",
        "cm = confusion_matrix(y_test, predicted_labels.numpy(), labels=[0, 1])\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "labels = [\"Small\", \"Large\"]\n",
        "sns.heatmap(cm_percent, annot=True, cmap='Purples', fmt='.1f', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-HUl_Q6TW7j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéÆ CHALLENGE: Minimize the loss as much as you can! ‚¨á\n",
        "\n",
        "\n",
        "IMPORTANT NOTE: You MUST re-run all of the cells starting from Step 1 (the one beginning with `model = MyNetwork()`) before you re-run the training loop after making any changes.\n",
        "\n",
        "Order of things to do:\n",
        "\n",
        "\n",
        "1.   Change a parameter listed below.\n",
        "2.   Do not run the training cell. Scroll up to Step 1, and run that one.\n",
        "3.   Then, run the Step 2 cell.\n",
        "4.   Then, run the Step 3 cell.\n",
        "\n",
        "This is important, because otherwise the model already has memory from the previous training, and we won't be able to assess what are the optimal parameter choices for the best results.\n",
        "\n",
        "\n",
        "Some things for you to try changing:\n",
        "* Number of epochs\n",
        "* Learning rate\n",
        "\n",
        "Make a note of which combinations you try, and what the lowest loss is at the end! Remember, we are aiming to get as close to zero as possible.\n",
        "\n",
        "Once you have a low loss, test the network and see what your accuracy is.\n",
        "\n",
        "Can you get an accuracy of 100%?\n",
        "\n",
        "See how the other plots change with your adapted parameters!"
      ],
      "metadata": {
        "id": "ScFTPYLRX3Mr"
      }
    }
  ]
}